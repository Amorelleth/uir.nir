\section{Фильтрация спама на основе машинного обучения}\label{Section:ML}
Алгоритмы машинного обучения достигли наибольшего успеха среди всех
перечисленных ранее методов, использованных в задаче фильтрации спама.
Сегодня самые успешные спам-фильтры используют машинное обучение \cite{filters}.

\subsection{Фильтрация спама как задача машинного обучения}
Автоматическая классификация электронной почты использует статистические подходы
или методы машинного обучения и направлена ​​на построение модели или классификатора
специально для задачи фильтрации спама из потока почты пользователей.

Задача фильтрация спама в контексте машинного обучения — это задача классификации,
в которой легитимные сообщения электронной почты рассматриваются как отрицательные
экземпляры, а спам — как положительные. Для построения модели или классификатора
требуется набор предварительно классифицированных документов. Процесс построения
модели называется обучением. Примеры, которые система использует для обучения,
называются обучающим набором \emph{(training set)}. Каждый обучающий при­мер называется
обучающим образцом \emph{(training instance)}.

Классификация является типичной задачей обучения с учителем. При обучении с учителем
обучающие данные, включают желательные решения, называемые метками. Алгоритм сначала
тренируется на объектах из обучающей выборки, для которых заранее известны метки классов.
Затем для каждого объекта из тестового набора данных \emph{(test set)} обученный алгоритм
предсказывает метку класса.

\subsection{Модели машинного обучения, используемые для фильтрации спама}\label{ML}

Успех алгоритмов машинного обучения в области фильтрации спама отчасти связан с тем, что
обучить и построить классификатор для сообщений электронной почты, получаемых отдельными
пользователями, легче,  чем создавать и настраивать набор правил фильтрации. Кроме того,
спам-фильтры на основе машинного обучения переобучаются при использовании и сводят к
минимуму ручные усилия.

\subsubsection{Наивный Байесовский классификатор (NBC)}
Классификатор был впервые разработан в 1990-х годах, и наиболее известен
своим применением как один из первых методов фильтрации спама \cite{Bayes}.
При Байесовском подходе максимизируется апостериорная вероятность класса.

В естественном языке вероятность появления определенного слова в письме
сильно зависит от контекста. Байесовский классификатор представляет письмо
как набор слов, вероятности появления которых условно не зависят друг от друга.
Такой подход также называется моделью "Мешок слов".
Исходя из предположения о независимости, условная вероятность принадлежности
письма к категории спама аппроксимируется произведением условных
вероятностей всех входящих в него слов.

Теорема Байеса применительно к задаче классификации спама имеет вид:
\begin{equation}\label{eq1}
    P(Class | WORD) = \frac{P(WORD | Class) \times P(Class)}{P(WORD)}
\end{equation}
где:
\begin{itemize}
    \item[—] ${«Class»}$ — либо ${«Spam»}$, либо ${«Ham»}$;
    \item[—] ${WORD}$ — это $({word_1}, {word_2}, \dots {word_n})$;
    \item[—] ${P(Class | WORD)}$ — условная вероятность того, что письмо принадлежит классу ${Class}$;
    \item[—] ${P(WORD | Class)}$ — вероятность обнаружить письмо среди всех писем класса ${Class}$;
    \item[—] ${P(Class)}$ — полная вероятность встретить письмо класса ${Class}$ в корпусе писем;
    \item[—] ${P(WORD)}$ — безусловная вероятность письма в наборе писем.
\end{itemize}

Предположение о независимости:
$P(WORD | Spam) \approx P(word_1 | Spam) \dots \\ P(word_n | Spam) = \prod_{i=1}^n P(word_i | Spam)$.

Если ${Class = Spam}$, уравнение \eqref{eq1} примет вид:


\begin{equation}\label{eq3}
    P(Spam | WORD) = \frac {\prod_{i=1}^n P(word_i | Spam) \times P(Spam)} {P(word_1, \dots ,word_n)}
\end{equation}


Существует три типа наивных байесовских классификаторов: мультиномиальные,
гауссовские и Бернулли. Для идентификации спама в электронной почте
был выбран полиномиальный наивный байесовский алгоритм, поскольку он
связан с текстом и превосходит по своим характеристикам гауссовский
алгоритм и алгоритм Бернулли \cite{IEEE}.

Несмотря на свои явно чрезмерно упрощенные предположения,
наивные байесовские классификаторы довольно хорошо работают
во многих реальных ситуациях, в том числе, при фильтрации
спама. Им также требуется небольшой объем обучающих
данных для оценки необходимых параметров \cite{scikitMNB}.

\subsubsection{Мультиномиальный наивный Байесовский \\ классификатор (MNB)}
В задачах классификации текста данные обычно представлены как счетчики векторов слов.
Распределение параметризуется векторами \\ $\theta_y = (\theta_{y1},\ldots,\theta_{yn})$
для каждого класса ${y}$, где ${n}$ - количество функций (в
классификации текста - размер словаря) и $\theta_{yi}$ это вероятность
$P(x_i \mid y)$ признака ${i}$ из выборки, принадлежащей к классу ${y}$.
Параметры $\theta_y$ оценивается сглаженной версией максимального правдоподобия,
то есть подсчетом относительной частоты:

\begin{equation}\label{eq4}
    \hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}

где:
\begin{itemize}
    \item[—] $N_{yi} = \sum_{x \in T} x_i$ — это количество раз,
        которое появляется признак ${i}$ в образце класса ${y}$;
    \item[—] $N_{y} = \sum_{i=1}^{n} N_{yi}$ — общее количество всех признаков для класса ${y}$.
\end{itemize}

Сглаживающий параметр $\alpha \ge 0$ учитывает особенности, отсутствующие
в обучающих выборках, и предотвращает нулевые вероятности в дальнейших
вычислениях. Параметр $\alpha = 1$ называется сглаживанием Лапласа, а $\alpha < 1$
называется сглаживанием Лидстоуна \cite{scikitMNB}.

\subsubsection{Метод опорных векторов (SVM)}
В отличие от наивного Байеса, SVM — не вероятностный алгоритм.

Гиперплоскость — пространство размерностью на единицу меньше размерности исходного пространства.
Основная цель метода опорных векторов в задаче бинарной классификации — найти уравнение разделяющей гиперплоскости $w_1x_1+w_2x_2+…+w_nx_n+w_0=0$
в пространстве $R^n$, которая бы разделила два класса неким оптимальным образом.

Возможные значения меток классов $Y = \{-1, +1\}$. Объект — вектор в пространстве $R^n$ c
N признаками $x = (x_1, x_2, \dots, x_n)$. Алгоритм при обучении должен построить функцию
$F(x)=y$, аргументом $x$ которой является объект из пространства $R^n$, а результатом — метка класса $y$.

Любая гиперплоскость может быть задана в виде $\langle w, x \rangle + b$.
Метод опорных векторов строит классифицирующую функцию

\begin{equation}\label{eq5}
    F(x) = sign(\langle w, x \rangle + b)
\end{equation}

где:
\begin{itemize}
    \item[—] $\langle , \rangle$ — скалярное произведение;
    \item[—] $w$ — нормальный вектор к разделяющей гиперплоскости;
    \item[—] $b$ — вспомогательный параметр.
\end{itemize}

Объекты, для которых $F(x) = 1$, оказываются по одну сторону гиперплоскости,
то есть попадают в один класс, а объекты с $F(x) = -1$ — по другую, соответственно,
попадают в другой класс.

$w$ и $b$ выбираются таким образом, чтобы максимизировать расстояние до каждого класса.
Другими словами, алгоритм максимизирует отступ (англ. \emph{margin}) между гиперплоскостью
и объектами классов, расположенными к ней ближе всего. Такие объекты называются опорными векторами \emph{(support vectors)}.
Расстояние до каждого класса равно $\frac {1}{\Arrowvert w \Arrowvert}$. Проблема нахождения максимума
$\frac {1}{\Arrowvert w \Arrowvert}$ эквивалентна проблеме нахождения минимума ${\Arrowvert w \Arrowvert}^2$.
Задача оптимизации:
\begin{equation}\label{eq6}
    \left\{ \begin{array}{ll} arg \: \underset{w,b}{\min} {\Arrowvert w \Arrowvert}^2 & \textrm{}\\ y_i(\langle w,x_i \rangle + b) \geqslant 1, \: i = 1, \dots, m \end{array} \right.
\end{equation}
решается с помощью множителей Лагранжа.

В случае линейной неразделимости, когда данные нельзя разделить гиперплоскостью, поступают
все элементы обучающей выборки вкладываются в пространство $X$ более высокой размерности с
помощью специального отображения $\varphi : R^n \rightarrow X$, которое выбирается таким образом,
чтобы выборка была линейно разделима в $X$.

Классифицирующая функция $F$ принимает вид $F(x)=sign(\langle w, \varphi (x) \rangle + b)$.
Ядро \emph{(kernel function)} классификатора: $k(x, x') = \langle \varphi (x), \varphi (x') \rangle $.
Ядром может служить любая положительно определенная симметричная функция двух переменных.
В разных алгоритмах SVM используются разные типы функций ядра. Например, линейная, нелинейная,
полиномиальная, радиальная базисная функция (RBF) и сигмоид \cite{SVM}.

Метод опорных векторов (SVM) считаются одним из лучших алгоритмов обучения
с учителем. Они обеспечивают превосходную производительность обобщения, требуют
меньше примеров для обучения и могут обрабатывать многомерные данные \cite{scikitSGD}.

\subsubsection{Стохастический градиентный спуск}\label{SGD}
На практике часто применяется метод опорных векторов, представляющий собой
линейную модель со стохастическим градиентным спуском (SGD).

Строго говоря, SGD — это просто метод оптимизации и не соответствует
конкретному семейству моделей машинного обучения \cite{IEEE}.

Этот алгоритм обеспечивает более точные результаты, чем сам SVM (алгоритм SVC в scikit-learn \cite{SVC}).
Недостатком работы с SVC является то, что он не может обрабатывать большой
набор данных, тогда как SGD обеспечивает эффективность и разнообразие
возможностей для настройки \cite{scikitSGD}.

\subsection{Выбор алгоритма машинного обучения для эксперимента}
Наивный байесовский классификатор использовался во многих приложениях обработки
информации, включая обработку естественного языка, поиск информации и т. д.
Данный метод особенно подходит для решения задач с большим объемом входных данных.

Однако на данный момент спамеры умеют легко обходить фильтр Байеса, просто дописывая в
конец письма много «легитимных» слов \cite{filters}. Этот метод обхода получил название «Отравление Байеса»,
а для фильтрации спама стали применять другие алгоритмы. Но наивный Байес навсегда остался
в учебниках как очень простой, красивый и один из первых приносящих практическую пользу.

Поэтому в эксперименте предпочтительнее использовать метод опорных векторов
со «стохастическим градиентным спуском». К тому же, большее количество параметров,
которые можно оптимизировать, предоставляют бо́льшие возможности для получения модели,
обеспечивающей наилучшую производительность, измеренную на проверочном наборе.

