\setcounter{chapter}{1}
\setcounter{section}{0}
\chapter*{Обзор алгоритмов фильтрации спама на основе машинного обучения}\label{Chapter:ML}
Алгоритмы машинного обучения достигли наибольшего успеха среди всех 
перечисленных ранее методов, использованных в задаче фильтрации спама 
\cite{filters}. Сегодня самые успешные спам-фильтры используют машинное обучение.
\section{Фильтрация спама как задача машинного обучения}
Автоматическая классификация электронной почты использует статистические подходы 
или методы машинного обучения и направлена ​​на построение модели или классификатора 
специально для задачи фильтрации спама из потока почты пользователей.

С точки зрения машинного обучения, фильтрация спама — это задача классификации, 
в которой легитимные сообщения электронной почты рассматриваются как отрицательные 
(-) экземпляры, а спам - как положительные (+). 
Для построения модели или классификатора требуется набор предварительно классифицированных 
документов (обучающий набор). Процесс построения модели называется обучением. 
В задаче бинарной классификации алгоритм МО сначала тренируется на объектах из 
обучающей выборки, для которых заранее известны метки классов. Затем для каждого 
объекта из тестового набора данных обученный алгоритм предсказывает метку класса. 

Успех алгоритмов машинного обучения в области фильтрации спама отчасти связан с тем, что 
обучить и построить классификатор для сообщений электронной почты, получаемых отдельными 
пользователями, легче,  чем создавать и настраивать набор правил фильтрации. Кроме того, 
спам-фильтры на основе машинного обучения переобучаются при использовании и сводят к 
минимуму ручные усилия.

% виды МО (с учителем наблюдателем и тд)

\section{Некоторые из популярных методов МО в фильтрации спама}
Для проведения эксперимента будет использована библиотека языка Python Scikit-Learn.
В дальнейшем программные скрипты будут реализованы с использованием методов 
оптимизации и сравнены с параметрами по умолчанию.

\subsection{Наивный Байесовский классификатор (NBC)}
Классификатор был впервые разработан в 1990-х годах, и наиболее известен 
своим применением как один из первых методов фильтрации спама. \cite{Bayes}
При Байесовском подходе максимизируется апостериорная вероятность класса.

В естественном языке вероятность появления определенного слова в письме 
сильно зависит от контекста. Байесовский классификатор представляет письмо 
как набор слов, вероятности появления которых условно не зависят друг от друга. 
Такой подход также называется моделью "Мешок слов". 
Исходя из предположения о независимости, условная вероятность принадлежности 
письма к категории спама аппроксимируется произведением условных 
вероятностей всех входящих в него слов.

Теорема Байеса применительно к задаче классификации спама, имеет вид:
\begin{equation}\label{eq1}
    P(Class | WORD) = \frac{P(WORD | Class) \times P(Class)}{P(WORD)}
\end{equation}
Где:
\begin{itemize}
    \item[\bullet] ${WORD}$ -- это $({word_1}, {word_2}, \dots {word_n})$;
    \item[\bullet] ${«Class»}$ -- либо ${«Spam»}$, либо ${«Ham»}$;
    \item[\bullet] ${P(Class | WORD)}$ -- условная вероятность того, что письмо принадлежит классу ${Class}$;
    \item[\bullet] ${P(WORD | Class)}$ -- вероятность обнаружить письмо среди всех писем класса ${Class}$;
    \item[\bullet] ${P(Class)}$ -- полная вероятность встретить письмо класса ${Class}$ в корпусе писем;
    \item[\bullet] ${P(WORD)}$ -- безусловная вероятность письма в корпусе писем;
\end{itemize}

Предположение о независимости:
\begin{equation}\label{eq2}
    P(WORD | Spam) \approx P(word_1 | Spam) \dots P(word_n | Spam) = \prod_{i=1}^n P(word_i | Spam)
\end{equation}

Если ${Class = Spam}$, уравнение \ref{eq1} примет вид:
\begin{equation}\label{eq3}
    P(Spam | WORD) = \frac {\prod_{i=1}^n P(word_i | Spam) \times P(Spam)} {P(word_1, \dots ,word_n)}
\end{equation}

Существует три типа наивных байесовских классификаторов: мультиномиальные, 
гауссовские и Бернулли. Для идентификации спама в электронной почте 
был выбран полиномиальный наивный байесовский алгоритм, поскольку он 
связан с текстом и превосходит по своим характеристикам гауссовский 
алгоритм и алгоритм Бернулли \cite{IEEE}.

Несмотря на свои явно чрезмерно упрощенные предположения, 
наивные байесовские классификаторы довольно хорошо работают 
во многих реальных ситуациях, в том сичле, при фильтрации 
спама. Им также требуется небольшой объем обучающих 
данных для оценки необходимых параметров. \cite{scikit}

\subsection{Мультиномиальный наивный Байесовский классификатор (MNB)}
В задачах классификации текста данные обычно представлены как счетчики векторов слов. 
Распределение параметризуется векторами $\theta_y = (\theta_{y1},\ldots,\theta_{yn})$ 
для каждого класса ${y}$, где ${n}$ - количество функций (в 
классификации текста - размер словаря) и $\theta_{yi}$ это вероятность 
$P(x_i \mid y)$ признака ${i}$ из выборки, принадлежащей к классу ${y}$.
Параметры $\theta_y$ оценивается сглаженной версией максимального правдоподобия, 
то есть подсчетом относительной частоты:

\begin{equation}\label{eq4}
    \hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}
\end{equation}

Где:
\begin{itemize}
    \item[\bullet] $N_{yi} = \sum_{x \in T} x_i$ -- это количество раз, 
    которое появляется признак ${i}$ в образце класса ${y}$
    \item[\bullet] $N_{y} = \sum_{i=1}^{n} N_{yi}$ -- общее количество всех признаков для класса ${y}$
\end{itemize}

Сглаживающий параметр $\alpha \ge 0$ учитывает особенности, отсутствующие 
в обучающих выборках, и предотвращает нулевые вероятности в дальнейших 
вычислениях. Параметр $\alpha = 1$ называется сглаживанием Лапласа, а $\alpha < 1$ 
называется сглаживанием Лидстоуна. \cite{scikit}

\subsection{Машины опорных векторов (SVM)}
В отличие от наивного Байеса, SVM -- не вероятностный алгоритм.

Гиперплоскость -- пространство размерностью на единицу меньше размерности исходного пространства.
Основная цель машины опорных векторов в задаче бинарной классификации —- найти 
уравнение разделяющей гиперплоскости $w_1x_1+w_2x_2+…+w_nx_n+w_0=0$ 
в пространстве $R^n$, которая бы разделила два класса неким оптимальным образом. 

Возможные значения меток классов $Y = \{-1, +1\}$. Объект —- вектор в пространстве $R^n$ c 
N признаками $x = (x_1, x_2, \dots, x_n)$. Алгоритм при обучении должен построить функцию 
$F(x)=y$, аргументом $x$ которой является объект из пространства $R^n$, а результатом -- метка класса $y$.

Любая гиперплоскость может быть задана в виде $\langle w, x \rangle + b$.
Метод опорных векторов строит классифицирующую функцию

% \begin{equation}\label{eq5}
    \[F(x) = sign(\langle w, x \rangle + b)\]
% \end{equation}

Где: 
\begin{itemize}
    \item[\bullet] $\langle , \rangle$ — скалярное произведение;
    \item[\bullet] $w$ -— нормальный вектор к разделяющей гиперплоскости;
    \item[\bullet] $b$ — вспомогательный параметр;
\end{itemize}

Объекты, для которых $F(x) = 1$, оказываются по одну сторону гиперплоскости, 
то есть попадают в один класс, а объекты с $F(x) = -1$ —- по другую, соответственно, 
попадают в другой класс.

$w$ и $b$ выбираются таким образом, чтобы максимизировать расстояние до каждого класса. 
Другими словами, алгоритм максимизирует отступ (англ. \emph{margin}) между гиперплоскостью 
и объектами классов, расположенными к ней ближе всего. Такие объекты называются опорными векторами \emph{(support vectors)}.
Расстояние до каждого класса равно $\frac {1}{\Arrowvert w \Arrowvert}$. Проблема нахождения максимума 
$\frac {1}{\Arrowvert w \Arrowvert}$ эквивалентна проблеме нахождения минимума ${\Arrowvert w \Arrowvert}^2$. 
Задача оптимизации:
\begin{equation}\label{eq6}
    \left\{ \begin{array}{ll} arg \: \underset{w,b}{\min} {\Arrowvert w \Arrowvert}^2 & \textrm{}\\ y_i(\langle w,x_i \rangle + b) \geqslant 1, \: i = 1, \dots, m \end{array} \right.
\end{equation}
решается с помощью множителей Лагранжа.

В случае линейной неразделимости, когда данные нельзя разделить гиперплоскостью, поступают 
все элементы обучающей выборки вкладываются в пространство $X$ более высокой размерности с 
помощью специального отображения $\varphi : R^n \rightarrow X$, которое выбирается таким образом, 
чтобы выборка была линейно разделима в $X$.

Классифицирующая функция $F$ принимает вид $F(x)=sign(\langle w, \varphi (x) \rangle + b)$. 
Ядро \emph{(kernel function)} классификатора: $k(x, x') = \langle \varphi (x), \varphi (x') \rangle $.
Ядром может служить любая положительно определенная симметричная функция двух переменных. 
В разных алгоритмах SVM используются разные типы функций ядра. Например, линейная, нелинейная, 
полиномиальная, радиальная базисная функция (RBF) и сигмоид. \cite{SVM}

Машины опорных векторов (SVM) считаются одним из лучших алгоритмов обучения 
с учителем. Они обеспечивают превосходную производительность обобщения, требуют 
меньше примеров для обучения и могут обрабатывать многомерные данные. \cite{scikit}

Гиперплоскость можно описать уравнением:
\begin{equation}\label{eq5}
    H = VX + c (5)
\end{equation}
где $c$ -- постоянная, а $V$ -- вектор.

Алгоритм использует скорость обучения для итерации по образец данных для 
оптимизации линейного алгоритма, и он обозначается следующим уравнением-6 
для скорости обучения по умолчанию как «Оптимальный»:

\section{Метрики машинного обучения}


\section{Выбор алгоритма МО для эксперимента}
Наивный байесовский классификатор (NB) - это простой, но эффективный классификатор, 
который использовался во многих приложениях обработки информации, включая обработку 
естественного языка, поиск информации и т. д. Данныцй метод особенно подходит для решения задач 
с большим объемом входных данных.

